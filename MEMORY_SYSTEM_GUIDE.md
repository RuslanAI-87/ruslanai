# Руководство по системе памяти RuslanAI

## Обзор проблемы

В текущей конфигурации замечена следующая проблема с системой памяти:

```
2025-05-17 03:44:18,209 - MemorySystem - WARNING - Векторное хранилище недоступно. Используется базовая система памяти.
```

Это приводит к тому, что центральный оркестратор не может сохранять контекст диалога между сообщениями.

## Причина проблемы

Анализ показывает, что векторное хранилище не инициализируется из-за отсутствия следующих зависимостей:
- sentence_transformers
- chromadb
- torch

В файле `central_agent/modules/memory/vector_store.py` система пытается инициализировать векторное хранилище, но не может загрузить необходимые библиотеки.

## Решение

Для решения проблемы необходимо выполнить следующие шаги:

1. **Установить зависимости для векторного хранилища**

   Запустите скрипт `setup_vector_store.bat`, который:
   - Установит все необходимые библиотеки
   - Создаст нужные директории
   - Протестирует векторное хранилище
   - Подготовит систему к использованию

2. **Перезапустить API-сервер**

   После установки зависимостей необходимо перезапустить API-сервер с помощью скрипта `restart_direct_api.bat`.

## Как проверить работу системы памяти

После установки зависимостей и перезапуска сервера можно проверить работу системы памяти следующим образом:

1. Откройте веб-интерфейс RuslanAI
2. Начните новый диалог и введите сообщение, например:
   ```
   Привет, меня зовут Алексей.
   ```
3. Затем введите вопрос, который требует знания контекста:
   ```
   Как меня зовут?
   ```
4. Если система ответит "Алексей", значит система памяти работает правильно

## Структура системы памяти

Система памяти RuslanAI состоит из нескольких компонентов:

1. **Иерархическая система памяти** (`memory_system.py`)
   - Обеспечивает многоуровневое хранение данных
   - Хранит память на трех уровнях: summary, context, detail
   - Использует векторное хранилище для семантического поиска

2. **Векторное хранилище** (`vector_store.py`)
   - Обеспечивает семантический поиск по содержимому
   - Поддерживает GPU-ускорение (если доступно)
   - Использует библиотеку sentence-transformers для создания эмбеддингов
   - Хранит данные в ChromaDB

## Особенности и оптимизация

1. **GPU-ускорение**
   - Если доступен GPU, система автоматически будет использовать его для ускорения создания эмбеддингов
   - Это значительно повышает скорость работы при больших объемах данных

2. **Контекст диалога**
   - После настройки система будет автоматически поддерживать контекст между сообщениями
   - Контекст хранится в долговременной памяти и не теряется при перезапуске

3. **Оптимизация использования памяти**
   - Информация хранится на разных уровнях детализации
   - Иерархическая система обеспечивает эффективное использование ресурсов

## Возможные проблемы и их решение

1. **Ошибка при установке зависимостей**
   - Убедитесь, что у вас установлен Python 3.8 или выше
   - Проверьте права доступа к директориям
   - Проверьте подключение к интернету

2. **Контекст всё равно не сохраняется**
   - Проверьте логи в директории `logs/vector_store.log`
   - Убедитесь, что API-сервер был перезапущен после установки зависимостей
   - Проверьте, что директория `data/vector_store` создана и доступна для записи

3. **Ошибка "Модель не найдена"**
   - Это может произойти при первом запуске, так как модель загружается с сервера
   - Убедитесь, что у вас есть доступ к интернету
   - Подождите несколько минут и попробуйте снова

## Дополнительная информация

Векторное хранилище использует модель `all-MiniLM-L6-v2` для создания эмбеддингов. Эта модель поддерживает 50+ языков, включая русский и английский, и предоставляет отличный баланс между качеством и скоростью работы.

При необходимости вы можете изменить модель в файле `central_agent/modules/memory/vector_store.py`, заменив строку:

```python
self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
```

на любую другую поддерживаемую модель из библиотеки sentence-transformers.